import os
from tempfile import TemporaryDirectory

from temporalio import activity

from nomad_actions.actions.entries.models import (
    CleanupArtifactsInput,
    ConsolidateOutputFilesInput,
    CreateArtifactSubdirectoryInput,
    ExportDatasetInput,
    SearchInput,
    SearchOutput,
)


@activity.defn
async def create_artifact_subdirectory(data: CreateArtifactSubdirectoryInput) -> str:
    """
    Creates a subdirectory within the action artifacts directory.

    Args:
        data (CreateArtifactSubdirectoryInput): Input data for creating subdirectory.

    Returns:
        str: Path to the created subdirectory.
    """
    from nomad.actions.manager import action_artifacts_dir

    subdir_path = os.path.join(action_artifacts_dir(), data.subdir_name)

    assert not os.path.exists(subdir_path)
    os.makedirs(subdir_path)

    return subdir_path


@activity.defn
async def search(data: SearchInput) -> SearchOutput:
    """
    Activity to perform NOMAD search based on the provided input data. The search
    results are written to a file in the specified format (Parquet, CSV, or JSON) in the
    artifacts directory.

    Args:
        data (SearchInput): Input data for the search activity.

    Returns:
        SearchOutput: Output data from the search activity.
    """
    from nomad.search import search as nomad_search

    from nomad_actions.actions.entries.utils import (
        write_csv_file,
        write_json_file,
        write_parquet_file,
    )

    logger = activity.logger

    output_file_extension = os.path.splitext(data.output_file_path)[-1]
    if output_file_extension == '.parquet':
        write_dataset_file = write_parquet_file
    elif output_file_extension == '.csv':
        write_dataset_file = write_csv_file
    elif output_file_extension == '.json':
        write_dataset_file = write_json_file
    else:
        raise ValueError(
            f'Unsupported file format "{output_file_extension}". Please use .parquet, '
            '.csv, or .json extensions.'
        )

    response = nomad_search(
        user_id=data.user_id,
        owner=data.owner,
        query=data.query,
        required=data.required,
        pagination=data.pagination,
        aggregations={},  # aggregations support can be added later
    )
    write_dataset_file(path=data.output_file_path, data=response.data)
    output = SearchOutput()
    if response.pagination and response.pagination.next_page_after_value:
        output.pagination_next_page_after_value = (
            response.pagination.next_page_after_value
        )
    logger.info(
        f'Page {response.pagination.page} containing {len(response.data)} results '
        f'written to output file {data.output_file_path}.'
    )

    return output


@activity.defn
async def consolidate_output_files(data: ConsolidateOutputFilesInput) -> str:
    """
    Activity to consolidate multiple Parquet, CSV, or JSON files into a single file.

    Args:
        data (ConsolidateOutputFilesInput): Input data for consolidating files.

    Returns:
        str: Path to the consolidated output file.
    """
    from nomad_actions.actions.entries.utils import consolidate_files

    consolidated_file_path = os.path.join(
        os.path.dirname(data.generated_file_paths[0]),
        'consolidated_dataset.' + data.generated_file_paths[0].split('.')[-1],
    )

    consolidate_files(data.generated_file_paths, consolidated_file_path)

    return consolidated_file_path


@activity.defn
async def export_dataset_to_upload(data: ExportDatasetInput) -> str:
    """
    Activity to export the generated dataset file to the specified upload.
    The dataset is generated by the previous activities in the artifacts directory.

    Args:
        data (ExportDatasetInput): Input data for exporting the dataset to the upload.
    Returns:
        str: Path to the saved dataset in the upload.
    """
    import shutil

    from nomad.actions.manager import get_upload_files

    upload_files = get_upload_files(data.upload_id, data.user_id)

    filename = os.path.basename(data.source_path)

    # check if filename already exists in the upload_files directory
    # if it does, append a part number to the filename
    # checks again until an available filename is found
    part_number = 1
    if upload_files.raw_path_exists(filename):
        while True:
            name, ext = os.path.splitext(filename)
            _filename = f'{name}({part_number}){ext}'
            if not upload_files.raw_path_exists(_filename):
                filename = _filename
                break
            part_number += 1

    # rename the dataset file to the available filename in a temporary directory
    # and upload it to the upload_files directory
    with TemporaryDirectory() as tmpdir:
        temp_file_path = os.path.join(tmpdir, filename)
        shutil.copy2(data.source_path, temp_file_path)
        upload_files.add_rawfiles(
            path=temp_file_path,
            auto_decompress=False,
        )

    return filename


@activity.defn
async def cleanup_artifacts(data: CleanupArtifactsInput) -> None:
    """
    Activity to clean up the action artifacts directory.

    Args:
        data (CleanupArtifactsInput): Input data for cleaning up artifacts.
    """
    import shutil

    if os.path.exists(data.subdir_path):
        shutil.rmtree(data.subdir_path)
